# üå≥ RAG Service - Intelligent Document Q&A with RAPTOR Trees

> **Modern RAG system** combining **RAGFlow retrieval**, **RAPTOR hierarchical trees**, and **Smart LLM chat** for intelligent document question-answering with multi-language support.

---

## üöÄ Overview

**RAG Service** is a production-ready system that transforms documents into intelligent Q&A capabilities:

- üìö **RAGFlow Processing**: Advanced document chunking and embedding with BGE-M3
- üå≥ **RAPTOR Trees**: Hierarchical clustering with GMM+BIC for optimal retrieval  
- ü§ñ **Smart Chat**: LLM-powered answers using Gemini with context awareness
- üè¢ **Multi-tenant**: Isolated data per tenant/knowledge base
- ‚ö° **High Performance**: 1-second query time with production optimizations

### Key Features

- ‚úÖ **Document Upload** - Process .md, .txt, .pdf files with intelligent chunking
- ‚úÖ **RAPTOR Tree Building** - Hierarchical clustering for enhanced context retrieval
- ‚úÖ **Smart Retrieval** - Hybrid search with vector + text + keyword matching  
- ‚úÖ **Intelligent Chat** - Context-aware LLM responses with safety checks
- ‚úÖ **Multi-language** - Auto-detect and respond in user's language (Vietnamese, English, Chinese)
- ‚úÖ **Production Ready** - Docker, async processing, comprehensive error handling

---

## üèóÔ∏è Architecture

```mermaid
flowchart TD
    A[Document Upload] --> B[RAGFlow Processing]
    B --> C[Chunking & Cleaning]
    C --> D[BGE-M3 Embeddings<br/>via Ollama]
    D --> E[RAPTOR Tree Building]
    E --> F[GMM Clustering + BIC]
    F --> G[DeepSeek-V3 Summary]
    G --> H[Supabase Storage]
    
    I[User Query] --> J[Smart Retrieval]
    J --> K[Vector Search + Ranking]
    K --> L[Context Preparation]
    L --> M[Gemini LLM]
    M --> N[Intelligent Answer]
    
    style D fill:#e1f5fe
    style G fill:#fff3e0
    style M fill:#f3e5f5
```

### Tech Stack

- **API**: FastAPI + Uvicorn (async)
- **Database**: Supabase (PostgreSQL + pgvector)  
- **Embeddings**: BGE-M3 via Ollama (local) or Voyage AI (cloud)
- **LLM Summary**: DeepSeek-V3 via FPT Cloud
- **LLM Chat**: Google Gemini 1.5 Flash
- **Clustering**: Gaussian Mixture Models + BIC optimization
- **Deployment**: Docker + Docker Compose

---

## üìñ API Endpoints

### Core APIs

| Endpoint | Method | Description | Use Case |
|----------|--------|-------------|----------|
| `/v1/ragflow/process` | POST | Upload & process documents into RAPTOR trees | Document ingestion |
| `/v1/ragflow/retrieve` | POST | Raw retrieval with chunks and similarity scores | Advanced retrieval |
| `/v1/chat/smart` | POST | **üåü Intelligent Q&A** - Auto retrieval + LLM generation | **Recommended for end users** |

### üåü Smart Chat API (Main Feature)

**Simplest way to get intelligent answers from your documents:**

```bash
curl --location 'http://localhost:8081/v1/chat/smart' \
--header 'Content-Type: application/json' \
--data '{
  "query": "Ng√†nh Thi·∫øt k·∫ø vi m·∫°ch b√°n d·∫´n c√≥ c∆° h·ªôi ngh·ªÅ nghi·ªáp nh∆∞ th·∫ø n√†o?",
  "tenant_id": "demo", 
  "kb_id": "docs"
}'
```

**Response:**
```json
{
  "answer": "Ng√†nh Thi·∫øt k·∫ø vi m·∫°ch b√°n d·∫´n c√≥ nhi·ªÅu c∆° h·ªôi ngh·ªÅ nghi·ªáp h·∫•p d·∫´n:\n\n**C∆° h·ªôi ngh·ªÅ nghi·ªáp:**\n- K·ªπ s∆∞ thi·∫øt k·∫ø vi m·∫°ch (v·ªã tr√≠ c·ªët l√µi)\n- Thi·∫øt k·∫ø m·∫°ch s·ªë, m·∫°ch t∆∞∆°ng t·ª±, m·∫°ch h·ªón h·ª£p\n- H·ªá th·ªëng tr√™n chip (SoC)\n- Ki·ªÉm th·ª≠ vi m·∫°ch\n\n**M·ª©c l∆∞∆°ng:**\n- Kh·ªüi ƒëi·ªÉm t·ª´ 12-20 tri·ªáu ƒë·ªìng/th√°ng\n- C√≥ th·ªÉ cao h∆°n t√πy theo nƒÉng l·ª±c v√† kinh nghi·ªám"
}
```

---

## ‚ö° Quick Start

### 1. Prerequisites

- **Python 3.10+**
- **Ollama** (for BGE-M3 embeddings)
- **Supabase** account (for database)

### 2. Install Dependencies

```bash
git clone <your-repo>
cd raptor_service

# Install with uv (recommended)
uv sync

# Or with pip
pip install -r requirements.txt
```

### 3. Setup Ollama & BGE-M3

```bash
# Install Ollama
curl -fsSL https://ollama.com/install.sh | sh

# Pull BGE-M3 model
ollama pull bge-m3:latest

# Verify
curl http://localhost:11434/api/tags
```

### 4. Configure Environment

Copy and configure environment file:
```bash
cp env.template .env
```

Edit `.env` with your credentials:
```env
# Supabase Database
DATABASE_URL=postgresql+psycopg://postgres.PROJECT_ID:PASSWORD@...supabase.com:6543/postgres?sslmode=require

# API Keys
GEMINI_API_KEY=your_gemini_api_key
LLM_API_KEY=your_fpt_cloud_api_key

# BGE-M3 via Ollama (default)
EMBED_BASE_URL=http://localhost:11434/api/embeddings
EMBED_MODEL=bge-m3:latest
```

### 5. Setup Database

```bash
python setup_database.py
```

**Expected output:**
```
üöÄ RAPTOR Service Database Setup
‚úÖ Database connection successful!
‚úÖ pgvector extension enabled
‚úÖ All required tables created
üéâ Database setup completed successfully!
```

### 6. Start Service

```bash
uvicorn main:app --reload --host 0.0.0.0 --port 8081
```

### 7. Upload Documents & Ask Questions

**Upload document:**
```bash
curl -X POST "http://localhost:8081/v1/ragflow/process" \
  -F "file=@your_document.md" \
  -F "tenant_id=demo" \
  -F "kb_id=docs"
```

**Ask questions:**
```bash
curl -X POST "http://localhost:8081/v1/chat/smart" \
  -H "Content-Type: application/json" \
  -d '{
    "query": "What is the main topic of the document?",
    "tenant_id": "demo",
    "kb_id": "docs"
  }'
```

---

## üõ†Ô∏è Configuration

### Environment Variables

| Variable | Required | Default | Description |
|----------|----------|---------|-------------|
| **Database** |
| `DATABASE_URL` | ‚úÖ | - | Supabase PostgreSQL connection string |
| `DB_ENABLE_SSL` | ‚ùå | `true` | Enable SSL for database connection |
| **Embeddings** |
| `EMBED_BASE_URL` | ‚úÖ | `http://localhost:11434/api/embeddings` | BGE-M3 via Ollama |
| `EMBED_MODEL` | ‚úÖ | `bge-m3:latest` | Embedding model name |
| `EMBED_VECTOR_DIM` | ‚úÖ | `1024` | Vector dimension |
| **LLM Services** |
| `GEMINI_API_KEY` | ‚úÖ | - | Google Gemini for smart chat |
| `LLM_API_KEY` | ‚úÖ | - | FPT Cloud for document summarization |
| `LLM_MODEL` | ‚ùå | `DeepSeek-V3` | Summary model |
| **Optional** |
| `RAPTOR_MAX_CLUSTERS` | ‚ùå | `64` | Max clusters per RAPTOR level |
| `CHUNK_SIZE` | ‚ùå | `1000` | Default chunk size |

### Alternative Embedding Services

**Switch to Voyage AI** (cloud-based):
```env
EMBED_BASE_URL=https://api.voyageai.com/v1
EMBED_API_KEY=your_voyage_api_keys_comma_separated
EMBED_MODEL=voyage-context-3
```

### Advanced Configuration

Detailed settings in `config/` directory:
- `config/database.py` - Database and connection pooling
- `config/embedding.py` - Embedding model configuration  
- `config/raptor.py` - RAPTOR tree building parameters
- `config/chat.py` - Gemini LLM settings

---

## üß™ Testing

### Run Test Suite
```bash
pytest tests/ -v
```

### Manual API Testing
```bash
# Test smart chat
python test_chat_simple.py

# Test document processing  
python test_knowledge_base_api.py

# Test with cURL
bash test_chat_curl.sh
```

### Health Check
```bash
curl http://localhost:8081/v1/chat/health
```

---

## üê≥ Docker Deployment

### Development
```bash
docker-compose -f docker-compose.dev.yml up -d
```

### Production
```bash
docker-compose up -d
```

**Service URLs:**
- API: `http://localhost:8081`
- Docs: `http://localhost:8081/docs`

---

## üìä Performance Metrics

### Retrieval Performance
- **Query Speed**: ~1 second for retrieval + generation
- **Embedding**: BGE-M3 1024-dimensional vectors
- **Tree Building**: GMM clustering with BIC optimization
- **Concurrency**: Async processing with connection pooling

### Scalability
- **Multi-tenant**: Isolated data per `tenant_id`/`kb_id`
- **Vector Search**: pgvector with HNSW indexing
- **Batch Processing**: Optimized embedding generation
- **Memory**: Efficient vector storage and retrieval

---

## üîß Troubleshooting

### Common Issues

**‚ùå Database Connection Failed**
```bash
# Test connection
python -c "from database.connection import test_connection; test_connection()"

# Check Supabase status
curl https://your-project.supabase.co/rest/v1/
```

**‚ùå Ollama/BGE-M3 Not Working**
```bash
# Check Ollama is running
curl http://localhost:11434/api/tags

# Restart Ollama
sudo systemctl restart ollama

# Re-pull model
ollama pull bge-m3:latest
```

**‚ùå Gemini API Errors**  
```bash
# Verify API key
python -c "import google.generativeai as genai; genai.configure(api_key='YOUR_KEY'); print('OK')"

# Check quota
curl -H "Authorization: Bearer $GEMINI_API_KEY" https://generativelanguage.googleapis.com/v1/models
```

**‚ùå No Context Retrieved**
```bash
# Check if documents are processed
curl -X POST "http://localhost:8081/v1/ragflow/retrieve" \
  -H "Content-Type: application/json" \
  -d '{"query": "test", "tenant_id": "demo", "kb_id": "docs"}'
```

### Debug Mode

Enable detailed logging:
```env
LOG_LEVEL=DEBUG
DB_DATABASE_ECHO=true
```

Check logs in `logs/` directory for detailed debugging information.

---

## üõ°Ô∏è Production Deployment

### Security Checklist
- [ ] Change default database credentials
- [ ] Enable SSL/TLS for all connections
- [ ] Rotate API keys regularly
- [ ] Set up proper firewall rules
- [ ] Enable request rate limiting

### Monitoring
- [ ] Set up health check endpoints
- [ ] Monitor database connection pool
- [ ] Track API response times
- [ ] Monitor embedding model performance

### Backup Strategy
- [ ] Regular database backups
- [ ] Document storage backup
- [ ] Environment configuration backup

---

## ü§ù Contributing

1. Fork the repository
2. Create feature branch: `git checkout -b feature/amazing-feature`
3. Follow code style: `black . && isort .`
4. Add tests for new features
5. Commit changes: `git commit -m 'Add amazing feature'`
6. Push to branch: `git push origin feature/amazing-feature`
7. Open a Pull Request

### Development Setup
```bash
# Install development dependencies
uv sync --dev

# Install pre-commit hooks
pre-commit install

# Run tests
pytest tests/ --cov=raptor_service
```

---

## üìÑ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## üôè Acknowledgments

- **RAPTOR**: Based on the RAPTOR paper for hierarchical retrieval (arXiv:2401.18059)
- **RAGFlow**: Inspired by RAGFlow's hybrid retrieval methodology
- **BGE-M3**: Beijing Academy of AI's multilingual embedding model
- **Supabase**: Modern PostgreSQL with vector extensions
- **Ollama**: Local LLM and embedding model serving
- **FPT Cloud**: Vietnamese LLM services
- **Google Gemini**: Advanced language model for chat

---

## üìû Support

- **Issues**: [GitHub Issues](https://github.com/your-repo/issues)
- **Discussions**: [GitHub Discussions](https://github.com/your-repo/discussions)
- **Documentation**: See `/docs` folder for detailed guides

---

*Built with ‚ù§Ô∏è for intelligent document Q&A - Supporting Vietnamese, English, and Chinese languages*