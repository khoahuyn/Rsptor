# ğŸŒ³ RAPTOR RAG Service - AI Assistants & Smart Document Q&A

> **Production-ready RAG system** with **RAPTOR hierarchical trees**, **AI Assistants**, and **React Frontend** for intelligent document processing and conversation management.

---

## ğŸš€ Overview

**RAPTOR RAG Service** transforms documents into intelligent AI assistants with conversation capabilities:

- ğŸ“š **RAGFlow Processing**: Advanced document chunking with parallel upload support
- ğŸŒ³ **RAPTOR Trees**: Hierarchical clustering with GMM+BIC for optimal retrieval  
- ğŸ¤– **AI Assistants**: Create and manage AI assistants linked to knowledge bases
- ğŸ’¬ **Chat Sessions**: Persistent conversation history with message management
- ğŸ¨ **React Frontend**: Complete UI for document upload, assistant creation, and chat
- ğŸ¢ **Multi-tenant**: Isolated data per tenant/knowledge base with cascade deletion

### â­ Key Features

- âœ… **AI Assistant Management** - Create, configure, and delete AI assistants
- âœ… **Chat Sessions & History** - Persistent conversations with message tracking
- âœ… **Parallel Document Upload** - Upload multiple .md files simultaneously with progress tracking
- âœ… **RAPTOR Tree Building** - Hierarchical clustering for enhanced context retrieval
- âœ… **Smart Retrieval** - Context-aware responses using knowledge base content
- âœ… **React Frontend** - Modern UI with real-time progress and error handling
- âœ… **Multi-language Support** - Auto-detect and respond in Vietnamese, English
- âœ… **Production Ready** - Async FastAPI, database migrations, comprehensive error handling

---

## ğŸ—ï¸ Architecture

```mermaid
flowchart TD
    subgraph "Frontend (React)"
        A[Document Upload UI] --> B[AI Assistant Creation]
        B --> C[Chat Interface]
    end
    
    subgraph "Backend (FastAPI)"
        D[RAGFlow Processing] --> E[RAPTOR Tree Building]
        E --> F[AI Assistant Management]
        F --> G[Chat Service]
    end
    
    subgraph "Embeddings"
        H[VoyageAI Multi-Key]
        I[BGE-M3 Local]
    end
    
    subgraph "Storage"
        J[(Supabase DB)]
    end
    
    A -->|Parallel Upload| D
    D --> J
    E --> H
    E --> I
    G --> J
    C -->|Real-time Chat| G
    
    style A fill:#e1f5fe
    style D fill:#fff3e0
    style G fill:#f3e5f5
    style H fill:#e8f5e8
    style J fill:#f1f8e9
```

### Tech Stack

- **Backend**: FastAPI + Uvicorn (async)
- **Frontend**: React + TypeScript + TanStack Router + Hero UI
- **Database**: Supabase (PostgreSQL + pgvector) with Alembic migrations
- **Embeddings**: 
  - **ğŸ† Recommended**: VoyageAI (multi-key, production-ready)
  - **Alternative**: BGE-M3 via Ollama (local, cost-effective)
- **LLM**: Gemini 1.5 Flash for chat + DeepSeek-V3 for summarization
- **Clustering**: Gaussian Mixture Models + BIC optimization

---

## ğŸ“– API Endpoints

### ğŸ¤– AI Assistant Management

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/v1/ai/assistants` | GET | List all AI assistants |
| `/v1/ai/assistants` | POST | Create new AI assistant |
| `/v1/ai/assistants/{id}` | PUT | Update AI assistant |
| `/v1/ai/assistants/{id}` | DELETE | Delete AI assistant (cascade) |

### ğŸ’¬ Chat & Sessions

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/v1/chat/sessions` | POST | Create new chat session |
| `/v1/chat/sessions/{id}/messages` | GET | Get session messages |
| `/v1/chat/sessions/{id}/chat` | POST | Send message & get AI response |
| `/v1/chat/assistants/{id}/sessions` | GET | Get assistant's chat sessions |

### ğŸ“š Document Processing

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/v1/ragflow/process` | POST | Upload & process documents (supports .md/.markdown) |
| `/v1/ragflow/process-optimized` | POST | ğŸš€ **Optimized processing** (recommended) |
| `/v1/ragflow/retrieve` | POST | Raw retrieval with chunks and scores |

### ğŸ—‚ï¸ Knowledge Base

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/v1/kb/create` | POST | Create knowledge base |
| `/v1/kb/list` | GET | List knowledge bases |
| `/v1/kb/{id}/documents` | GET | Get documents in KB |

---

## âš¡ Quick Start

### 1. Prerequisites

- **Python 3.10+**
- **Node.js 18+** (for frontend)
- **VoyageAI API Key** (recommended) or **Ollama** (for local BGE-M3)
- **Supabase** account (for database)

### 2. Backend Setup

```bash
git clone <your-repo>
cd raptor_service

# Install dependencies
pip install -r requirements.txt

# Setup environment
cp env.template .env
# Edit .env with your credentials (see Configuration section)

# Setup database
python setup_database.py

# Start backend
uvicorn main:app --reload --host 0.0.0.0 --port 8081
```

### 3. Frontend Setup

```bash
cd frontend

# Install dependencies
npm install

# Start development server  
npm run dev
```

### 4. Access Applications

- **Backend API**: http://localhost:8081
- **API Documentation**: http://localhost:8081/docs
- **Frontend UI**: http://localhost:5173

---

## ğŸ”§ Configuration

### ğŸ† Recommended: VoyageAI Multi-Key Setup

```env
# VoyageAI Embeddings (RECOMMENDED for production)
EMBED_BASE_URL=https://api.voyageai.com/v1
EMBED_API_KEY=pa-key1,pa-key2,pa-key3,pa-key4,pa-key5,pa-key6
EMBED_MODEL=voyage-context-3
EMBED_VECTOR_DIM=1024

# Benefits:
# âœ… Fastest performance with parallel processing
# âœ… Multi-key parallel processing (4-6 keys recommended)
# âœ… Professional-grade reliability
# âœ… Zero rate limits with proper key distribution
```

### ğŸ”„ Alternative: BGE-M3 Local Setup

```env
# BGE-M3 Local Embeddings (cost-effective)
EMBED_BASE_URL=http://localhost:11434/api/embeddings
EMBED_API_KEY=
EMBED_MODEL=bge-m3:latest
EMBED_VECTOR_DIM=1024
```

**Setup Ollama for BGE-M3:**
```bash
# Install Ollama
curl -fsSL https://ollama.com/install.sh | sh

# Pull BGE-M3 model
ollama pull bge-m3:latest

# Start Ollama service
ollama serve

# Verify
curl http://localhost:11434/api/tags
```

### Complete Environment Variables

```env
# === EMBEDDING CONFIGURATION ===
# VoyageAI (RECOMMENDED - fastest performance)
EMBED_BASE_URL=https://api.voyageai.com/v1
EMBED_API_KEY=pa-key1,pa-key2,pa-key3,pa-key4,pa-key5,pa-key6
EMBED_MODEL=voyage-context-3
EMBED_VECTOR_DIM=1024

# Alternative: BGE-M3 Local (cost-effective)
# EMBED_BASE_URL=http://localhost:11434/api/embeddings
# EMBED_API_KEY=
# EMBED_MODEL=bge-m3:latest
# EMBED_VECTOR_DIM=1024

# === LLM CONFIGURATION ===
# FPT Cloud for document summarization
LLM_BASE_URL=https://mkp-api.fptcloud.com/v1
LLM_API_KEY=your_fpt_cloud_api_key
LLM_MODEL=DeepSeek-V3

# Google Gemini for smart chat
GEMINI_API_KEY=your_gemini_api_key

# === DATABASE CONFIGURATION ===
# Supabase PostgreSQL with pgvector
DATABASE_URL=postgresql+psycopg://postgres.PROJECT_ID:PASSWORD@aws-0-ap-southeast-1.pooler.supabase.com:6543/postgres?sslmode=require
DB_ENABLE_SSL=true
DB_SSL_CERT_PATH=prod-ca-2021.crt
SUPABASE_SSLROOTCERT=./database/prod-ca-2021.crt

# === OPTIONAL CONFIGURATION ===
# Raptor tree building
RAPTOR_MAX_CLUSTERS=64
RAPTOR_THRESHOLD=0.1
RAPTOR_RANDOM_SEED=42

# Chunking
CHUNK_SIZE=1000
CHUNK_OVERLAP=200

# API settings
API_HOST=0.0.0.0
API_PORT=8081
API_RELOAD=true
```

### ğŸ”‘ VoyageAI Multi-Key Best Practices

**âœ… Recommended Key Count:** 4-6 keys
```env
# Example with multiple keys for optimal performance
EMBED_API_KEY=pa-xxxxxxxxxx1,pa-xxxxxxxxxx2,pa-xxxxxxxxxx3,pa-xxxxxxxxxx4,pa-xxxxxxxxxx5,pa-xxxxxxxxxx6
```

**Benefits of Multiple Keys:**
- ğŸš€ **Parallel Processing**: True parallel embedding across all keys
- âš¡ **Zero Rate Limits**: Smart load balancing prevents rate limit encounters
- ğŸ¯ **Optimal Performance**: Significantly faster than sequential processing
- ğŸ›¡ï¸ **Automatic Failover**: Rapid failover if any key encounters issues
- ğŸ“Š **Token-aware Selection**: Prioritizes keys with lower current usage

---

## ğŸ’» Frontend Features

### ğŸ“ Document Management
- **Parallel Upload**: Upload multiple .md files simultaneously
- **Progress Tracking**: Real-time upload progress with percentage
- **Validation**: Client-side validation for file types and sizes
- **Error Handling**: Detailed error messages and retry capability

### ğŸ¤– AI Assistant Creation
- **Knowledge Base Integration**: Link assistants to specific knowledge bases
- **Custom Configuration**: Set system prompts and model parameters
- **Management**: Full CRUD operations with cascade deletion

### ğŸ’¬ Chat Interface
- **Real-time Messaging**: Send messages and receive AI responses
- **Session Management**: Create and switch between chat sessions
- **Message History**: Persistent conversation storage
- **Context-aware Responses**: AI responses based on knowledge base content

---

## ğŸ§ª Usage Examples

### Create AI Assistant

```bash
curl -X POST "http://localhost:8081/v1/ai/assistants" \
  -H "Content-Type: application/json" \
  -d '{
    "tenant_id": "demo",
    "kb_id": "my_documents",
    "name": "Technical Support Bot",
    "description": "AI assistant for technical documentation",
    "system_prompt": "You are a helpful technical support assistant."
  }'
```

### Upload Documents (Optimized)

```bash
curl -X POST "http://localhost:8081/v1/ragflow/process-optimized" \
  -F "file=@technical_guide.md" \
  -F "tenant_id=demo" \
  -F "kb_id=my_documents" \
  -F "enable_raptor=true"
```

### Start Chat Session

```bash
curl -X POST "http://localhost:8081/v1/chat/sessions" \
  -H "Content-Type: application/json" \
  -d '{
    "tenant_id": "demo",
    "assistant_id": "demo::assistant::12345",
    "name": "Support Chat"
  }'
```

### Send Message

```bash
curl -X POST "http://localhost:8081/v1/chat/sessions/{session_id}/chat" \
  -H "Content-Type: application/json" \
  -d '{
    "message": "How do I configure the database connection?",
    "stream": false
  }'
```

---

## ğŸ“Š Performance & Scalability

### ğŸ† RAPTOR Processing Performance

**VoyageAI Multi-Key (Recommended):**
```bash
âœ… RAPTOR Tree Building: Fast and reliable
âœ… Embedding Generation: Near-instant with parallel processing
âœ… Total Processing: Optimized for production workloads
âœ… Parallel Keys: Multiple keys simultaneous processing
âœ… Rate Limits: Zero encounters with proper load balancing
```

**BGE-M3 Local (Optimized):**
```bash
âœ… RAPTOR Tree Building: Competitive performance
âœ… Embedding Generation: Fast with optimized parallel processing
âœ… Total Processing: Efficient for cost-conscious deployments
âœ… Parallel Processing: High concurrent connections
âœ… Cost: Zero API costs
```

### Performance Comparison

| Feature | VoyageAI Multi-Key | BGE-M3 Local |
|---------|-------------------|--------------|
| **RAPTOR Speed** | Fastest âš¡ | Fast âœ… |
| **Embedding Speed** | Excellent ğŸš€ | Good âœ… |
| **Reliability** | Production-grade ğŸ›¡ï¸ | Reliable ğŸ›¡ï¸ |
| **Cost** | API usage ğŸ’³ | Free ğŸ†“ |
| **Privacy** | Cloud â˜ï¸ | Local ğŸ”’ |

### Database Features
- **Multi-tenant Isolation**: Data separated by `tenant_id`
- **Vector Search**: pgvector with HNSW indexing for fast similarity search
- **Optimized Writes**: Bulk operations with skip-refresh for embeddings
- **Cascade Deletion**: Automatic cleanup when deleting assistants
- **Migration System**: Alembic for database schema management

### Frontend Performance  
- **Parallel Processing**: Multiple file uploads with individual progress tracking
- **Real-time Updates**: Immediate UI updates without page refresh
- **Error Recovery**: Graceful handling of upload failures and retries
- **Responsive Design**: Works on desktop and mobile devices

---

## ğŸ”§ Troubleshooting

### Common Issues

**âŒ Database Connection Failed**
```bash
# Test connection
python setup_database.py

# Check Supabase project status and credentials in .env
```

**âŒ VoyageAI API Issues**
```bash
# Check API keys format
# Ensure keys start with "pa-" and are comma-separated
# Verify API key permissions and rate limits
```

**âŒ Ollama/BGE-M3 Not Working**
```bash
# Check Ollama service
curl http://localhost:11434/api/tags

# Restart and pull model
ollama pull bge-m3:latest
```

**âŒ Frontend Not Loading**
```bash
cd frontend
npm install
npm run dev

# Check if backend is running on port 8081
```

**âŒ File Upload Fails**
- Ensure files are .md or .markdown format
- Check file size limits (10MB default)
- Verify knowledge base exists before upload

### Debug Mode

Enable detailed logging:
```env
LOG_LEVEL=DEBUG
```

Check browser console for frontend issues and backend logs for API problems.

---

## ğŸš€ Production Deployment

### Backend Deployment
1. Set production environment variables (use VoyageAI for best performance)
2. Run database migrations: `alembic upgrade head`
3. Deploy FastAPI with proper ASGI server (Gunicorn + Uvicorn)
4. Configure reverse proxy (Nginx) for static files and API

### Frontend Deployment
```bash
cd frontend
npm run build
# Deploy dist/ folder to static hosting (Vercel, Netlify, etc.)
```

### Security Checklist
- [ ] Use environment variables for all secrets
- [ ] Enable HTTPS for all connections
- [ ] Configure CORS for production domains
- [ ] Set up proper database user permissions
- [ ] Regular backup of database and uploaded files
- [ ] Rotate VoyageAI API keys regularly

### Production Recommendations
- **ğŸ† Use VoyageAI**: Best performance and reliability
- **ğŸ“Š Monitor Usage**: Track API usage and costs
- **ğŸ”„ Load Balancing**: Consider multiple backend instances
- **ğŸ“ˆ Scaling**: Add more VoyageAI keys for higher throughput

---

## ğŸ¤ Contributing

1. Fork the repository
2. Create feature branch: `git checkout -b feature/amazing-feature`  
3. Follow code style: `black . && isort .`
4. Test both backend and frontend changes
5. Commit changes: `git commit -m 'Add amazing feature'`
6. Push to branch: `git push origin feature/amazing-feature`
7. Open a Pull Request

### Development Setup

```bash
# Backend development
pip install -r requirements.txt
python setup_database.py
uvicorn main:app --reload

# Frontend development  
cd frontend
npm install
npm run dev

# Run tests
pytest tests/ -v
```

---

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ™ Acknowledgments

- **RAPTOR**: Based on the RAPTOR paper for hierarchical retrieval (arXiv:2401.18059)
- **RAGFlow**: Inspired by RAGFlow's hybrid retrieval methodology
- **VoyageAI**: Advanced embedding APIs with excellent performance
- **BGE-M3**: Beijing Academy of AI's multilingual embedding model
- **Supabase**: Modern PostgreSQL with vector extensions
- **Ollama**: Local embedding model serving
- **React + Hero UI**: Modern frontend framework and component library

---

*Built with â¤ï¸ for intelligent document processing and AI assistant management*

*ğŸ† Optimized for production with VoyageAI multi-key embedding and RAPTOR hierarchical trees*