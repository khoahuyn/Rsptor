# üå≥ RAPTOR RAG Service - Pure Retrieval & Smart Citations

> **Production-ready RAG system** with **RAPTOR hierarchical trees**, **DeepSeek-R1 + Gemini dual LLM setup**, and **intelligent model-driven citations** for smart document Q&A.

---

## üöÄ Overview

**RAPTOR RAG Service** provides pure retrieval capabilities with smart citation system:

- üìö **RAGFlow Processing**: Advanced document chunking with parallel upload support
- üå≥ **RAPTOR Trees**: Hierarchical clustering with GMM+BIC for optimal retrieval  
- ü§ñ **Dual LLM Setup**: DeepSeek-R1 (primary) + Gemini (fallback) for reliable chat responses
- üí° **Smart Citations**: Model-driven citation extraction based on LLM thinking process
- üéØ **Pure Retrieval**: MCP-ready architecture without built-in multi-agent orchestration
- üè¢ **Multi-tenant**: Isolated data per tenant/knowledge base

### ‚≠ê Key Features

- ‚úÖ **DeepSeek-R1 Integration** - Primary LLM with thinking extraction for citations
- ‚úÖ **Intelligent Citations** - Model decides what to cite based on internal reasoning  
- ‚úÖ **Dual LLM Strategy** - Primary/fallback system for reliable responses
- ‚úÖ **Pure Retrieval Design** - Clean separation from multi-agent orchestration
- ‚úÖ **Model-Driven Excerpts** - Citations based on what model actually mentioned
- ‚úÖ **Dynamic Citation Logic** - No hardcoded patterns, adapts to model behavior
- ‚úÖ **Clean Architecture** - Simplified codebase with reduced complexity

### üß† **Citation Intelligence**

**Model-Driven Citation System:**
- üéØ **Thinking-Based Citations** - System analyzes LLM's `<think>` content to decide when to show citations
- üìù **Dynamic Excerpt Selection** - Extracts relevant excerpts based on what model described  
- üö´ **Smart Filtering** - No citations for "no information available" responses
- ‚ö° **Answer-Aware Scoring** - Prioritizes content chunks that match model's actual answer
- üßπ **No Hardcoded Patterns** - Completely dynamic without fixed keywords or rules

---

## üèóÔ∏è Architecture

```mermaid
flowchart TD
    subgraph "LLM Strategy"
        A[DeepSeek-R1 Primary] --> B{Success?}
        B -->|Yes| C[Extract Thinking]
        B -->|No| D[Gemini Fallback]
        D --> C
    end
    
    subgraph "Citation Intelligence" 
        C --> E[Analyze Thinking Content]
        E --> F{Model Referenced Chunks?}
        F -->|Yes| G[Show Smart Citations]
        F -->|No| H[No Citations]
    end
    
    subgraph "Retrieval Core"
        I[RAPTOR Tree Search]
        J[Chunk Prioritization]  
        K[Content Matching]
    end
    
    subgraph "Storage"
        L[(Supabase DB)]
        M[(Vector Index)]
    end
    
    I --> J
    J --> K
    K --> A
    G --> L
    
    style A fill:#e1f5fe
    style D fill:#fff3e0
    style E fill:#f3e5f5
    style G fill:#e8f5e8
```

### Tech Stack

- **Backend**: FastAPI + Uvicorn (async)
- **Database**: Supabase (PostgreSQL + pgvector) 
- **Embeddings**: VoyageAI (multi-key) or BGE-M3 (local)
- **Primary LLM**: DeepSeek-R1 via FPT Cloud API 
- **Fallback LLM**: Google Gemini 1.5 Flash
- **Clustering**: Gaussian Mixture Models + BIC optimization
- **Vector Search**: FAISS with similarity ranking

---

## ü§ñ LLM Configuration

### DeepSeek-R1 + Gemini Setup

```env
# Primary LLM: DeepSeek-R1 (via FPT Cloud)
LLM_BASE_URL=https://mkp-api.fptcloud.com/v1  
LLM_API_KEY=your_fpt_cloud_api_key
PRIMARY_CHAT_MODEL=deepseek-r1
PRIMARY_CHAT_TEMPERATURE=0.7
PRIMARY_CHAT_MAX_TOKENS=2048

# Fallback LLM: Google Gemini
GEMINI_API_KEY=your_gemini_api_key
GEMINI_MODEL=gemini-1.5-flash
GEMINI_TEMPERATURE=0.7
GEMINI_MAX_OUTPUT_TOKENS=2048
```

### Citation Behavior

**When Citations Are Shown:**
- ‚úÖ Model's thinking contains chunk references (`chunk_21`, `chunk_28`)  
- ‚úÖ Model mentions specific facts from content (`"it states that..."`, `"mentions that..."`)
- ‚úÖ Model answer overlaps with retrieved content

**When Citations Are Hidden:**
- ‚ùå Model states "no information available" or similar
- ‚ùå Model doesn't reference specific chunks in thinking
- ‚ùå Generic responses not based on retrieved content

---

## üìñ API Endpoints

### üí¨ Chat & Retrieval

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/v1/chat/assistant` | POST | Chat with AI assistant (DeepSeek-R1 ‚Üí Gemini fallback) |
| `/v1/ragflow/retrieve` | POST | Pure retrieval with RAPTOR trees |

### ü§ñ AI Assistant Management

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/v1/ai/assistants` | GET | List all AI assistants |
| `/v1/ai/assistants` | POST | Create new AI assistant |
| `/v1/ai/assistants/{id}` | DELETE | Delete AI assistant (cascade) |

### üìö Document Processing

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/v1/ragflow/process` | POST | Upload & process documents (supports .md/.markdown) |
| `/v1/kb/create` | POST | Create knowledge base |
| `/v1/kb/list` | GET | List knowledge bases |

---

## ‚ö° Quick Start

### 1. Prerequisites

- **Python 3.10+**
- **uv** (fast Python package manager)
- **FPT Cloud API Key** (for DeepSeek-R1)
- **Google Gemini API Key** (for fallback)
- **VoyageAI API Key** (recommended) or **Ollama** (for local embeddings)
- **Supabase** account (for database)

### 2. Backend Setup

```bash
git clone <your-repo>
cd raptor_service

# Install dependencies with uv
uv pip install -r requirements.txt

# Setup environment
cp env.template .env
# Edit .env with your API keys (see Configuration section)

# Setup database
python setup_database.py

# Start backend
uvicorn main:app --reload --host 0.0.0.0 --port 8081
```

### 3. Access Applications

- **Backend API**: http://localhost:8081
- **API Documentation**: http://localhost:8081/docs

---

## üîß Configuration

### Complete Environment Variables

```env
# === DUAL LLM CONFIGURATION ===
# Primary LLM: DeepSeek-R1 (via FPT Cloud)
LLM_BASE_URL=https://mkp-api.fptcloud.com/v1
LLM_API_KEY=your_fpt_cloud_api_key
PRIMARY_CHAT_MODEL=deepseek-r1
PRIMARY_CHAT_TEMPERATURE=0.7
PRIMARY_CHAT_MAX_TOKENS=2048

# Fallback LLM: Google Gemini  
GEMINI_API_KEY=your_gemini_api_key
GEMINI_MODEL=gemini-1.5-flash
GEMINI_TEMPERATURE=0.7
GEMINI_MAX_OUTPUT_TOKENS=2048

# === EMBEDDING CONFIGURATION ===
# VoyageAI (RECOMMENDED - multi-key support)
EMBED_BASE_URL=https://api.voyageai.com/v1
EMBED_API_KEY=pa-key1,pa-key2,pa-key3,pa-key4
EMBED_MODEL=voyage-context-3
EMBED_VECTOR_DIM=1024

# Alternative: BGE-M3 Local (cost-effective)
# EMBED_BASE_URL=http://localhost:11434/api/embeddings
# EMBED_API_KEY=
# EMBED_MODEL=bge-m3:latest
# EMBED_VECTOR_DIM=1024

# === DATABASE CONFIGURATION ===
DATABASE_URL=postgresql+psycopg://postgres.PROJECT_ID:PASSWORD@aws-0-ap-southeast-1.pooler.supabase.com:6543/postgres?sslmode=require
DB_ENABLE_SSL=true
SUPABASE_SSLROOTCERT=./database/prod-ca-2021.crt

# === OPTIONAL CONFIGURATION ===
# Raptor tree building
RAPTOR_MAX_CLUSTERS=64
RAPTOR_THRESHOLD=0.1
RAPTOR_RANDOM_SEED=42

# Chunking
CHUNK_SIZE=1000
CHUNK_OVERLAP=200

# API settings
API_HOST=0.0.0.0
API_PORT=8081
API_RELOAD=true
```

---

## üß™ Usage Examples

### Chat with Smart Citations

```bash
curl -X POST "http://localhost:8081/v1/chat/assistant" \
  -H "Content-Type: application/json" \
  -d '{
    "query": "What team won the 2019 League of Legends World Championship?",
    "session_id": "demo::session::12345"
  }'
```

**Response with Smart Citations:**
```json
{
  "answer": "FunPlus Phoenix (FPX) won the 2019 League of Legends World Championship, defeating G2 Esports 3-0 in the finals.",
  "context_passages": [
    {
      "source_id": "doc1_chunk_25",
      "relevant_excerpt": "**2019** saw FunPlus Phoenix secure another Chinese victory over G2 Esports (3-0) with Tian earning MVP honors.",
      "similarity_score": 0.89
    }
  ]
}
```

### Pure Retrieval

```bash
curl -X POST "http://localhost:8081/v1/ragflow/retrieve" \
  -H "Content-Type: application/json" \
  -d '{
    "query": "RAPTOR clustering algorithm",
    "tenant_id": "demo",
    "kb_id": "technical_docs",
    "top_k": 5
  }'
```

### Upload Documents

```bash
curl -X POST "http://localhost:8081/v1/ragflow/process" \
  -F "file=@technical_guide.md" \
  -F "tenant_id=demo" \
  -F "kb_id=my_documents" \
  -F "enable_raptor=true"
```

---

## üîç Citation System Details

### How It Works

1. **User Query** ‚Üí RAPTOR retrieval finds relevant chunks
2. **DeepSeek-R1** processes query + chunks ‚Üí generates answer with `<think>` content  
3. **Citation Logic** analyzes thinking:
   - If thinking mentions `chunk_X` ‚Üí show citations from those chunks
   - If thinking has "no information" ‚Üí hide citations
   - Extract relevant excerpts based on model's descriptions
4. **Fallback to Gemini** if DeepSeek-R1 fails

### Example Thinking Analysis

**DeepSeek-R1 Thinking:**
```
<think>
Looking at chunk_21, it mentions that Edward Gaming won in 2021. 
Chunk_28 also states they defeated DWG KIA 3-2 in the finals.
</think>
```

**System Response:**
- ‚úÖ **Citations shown** (model referenced chunk_21, chunk_28)
- üéØ **Prioritized chunks 21 & 28** in citation display  
- üìù **Smart excerpts** based on "won in 2021" and "defeated DWG KIA 3-2"

---

## üèóÔ∏è Architecture Changes

### What's New

- **üîÑ Removed Deep Research**: Simplified to pure retrieval for MCP compatibility
- **ü§ñ Dual LLM Strategy**: DeepSeek-R1 primary, Gemini fallback  
- **üß† Thinking Extraction**: Parse `<think>` tags for citation intelligence
- **üìù Model-Driven Citations**: Let LLM decide what's relevant to cite
- **üßπ Code Cleanup**: Removed 190+ lines of hardcoded patterns
- **üéØ Dynamic Logic**: No fixed keywords, adapts to model behavior

### What Was Removed

- ‚ùå **Deep Research Module** - Multi-step reasoning (moved to external orchestration)
- ‚ùå **Hardcoded Citation Patterns** - Fixed regex patterns for excerpt extraction  
- ‚ùå **Complex Scoring Algorithms** - Overcomplicated factual marker bonuses
- ‚ùå **Fixed Keyword Lists** - Stop words, team names, action verbs hardcoded
- ‚ùå **Generic Fallbacks** - Complex algorithm-based excerpt selection

### Code Quality Improvements

```bash
# Before: Complex hardcoded approach
thinking_helper.py: 258 lines with hardcoded patterns
citation_formatter.py: 180+ lines with complex scoring

# After: Clean model-driven approach  
thinking_helper.py: 88 lines, pure dynamic logic
citation_formatter.py: Simplified with model-guided excerpts

üéØ Result: 60% less code, 100% more reliable citations
```

---

## üîß Troubleshooting

### Common Issues

**‚ùå DeepSeek-R1 API Issues**
```bash
# Check FPT Cloud API key and credits
# Verify MODEL_NAME matches exactly: "deepseek-r1"  
# System will fallback to Gemini automatically
```

**‚ùå Citations Not Showing**
```bash
# Check if model mentioned chunks in thinking
# Look for patterns like "chunk_21 mentions..." in logs
# Verify model answer overlaps with retrieved content
```

**‚ùå Database Connection Failed**
```bash
# Test connection
python setup_database.py

# Check Supabase project status and credentials in .env
```

### Debug Mode

Enable detailed logging:
```env
LOG_LEVEL=INFO
```

Check logs for:
- `ü§ñ Trying DeepSeek-R1 (Primary LLM)`
- `üß† Thinking extracted: X chars`  
- `üéØ Model described: '...' ‚Üí best match`
- `üìä Citation decision: True/False`

---

## üöÄ Production Deployment

### Security Checklist
- [ ] Use environment variables for all API keys
- [ ] Enable HTTPS for all connections
- [ ] Configure CORS for production domains
- [ ] Set up proper database user permissions
- [ ] Regular backup of database and files
- [ ] Rotate FPT Cloud & Gemini API keys regularly

### Production Recommendations
- **ü§ñ Monitor API Usage**: Track DeepSeek-R1 & Gemini usage/costs
- **üîÑ Load Balancing**: Consider multiple backend instances
- **üìä Citation Quality**: Monitor citation relevance in production
- **‚ö° Fallback Monitoring**: Track primary vs fallback LLM usage rates

---

## ü§ù Contributing

1. Fork the repository
2. Create feature branch: `git checkout -b feature/citation-improvements`
3. Follow code style: `black . && isort .`
4. Test citation system with various queries
5. Commit changes: `git commit -m 'Improve citation extraction'`
6. Push to branch: `git push origin feature/citation-improvements`
7. Open a Pull Request

### Development Setup

```bash
# Backend development (using uv)
uv pip install -r requirements.txt
python setup_database.py
uvicorn main:app --reload

# Run tests
pytest tests/ -v
```

---

## üìÑ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## üôè Acknowledgments

- **RAPTOR**: Based on the RAPTOR paper for hierarchical retrieval (arXiv:2401.18059)
- **DeepSeek-R1**: Advanced reasoning LLM with thinking capabilities
- **VoyageAI**: High-quality embedding APIs
- **Google Gemini**: Reliable fallback LLM
- **Supabase**: Modern PostgreSQL with vector extensions

---

*Built with ‚ù§Ô∏è for intelligent document retrieval and smart AI citations*

*üß† **Model-driven citations** - Let the AI decide what's worth citing*

*üéØ **Pure retrieval architecture** - MCP-ready for external orchestration*